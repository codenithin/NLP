{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f12bf5",
   "metadata": {},
   "source": [
    "This is the Text mining notebook of Nithin Kumar Nadagouda.\n",
    "The main purpose of this text mining notebook is to understand and master the Natural language Processing.\n",
    "The sources of the knowledge is going to be the CodeAcademy Course(Getting Started with Natural Language Processing) and various other internet sources.\n",
    "\n",
    "The learning method will be to utilize the code available in CodeAcademy and internet sources to clearly understand the Text Analysis and various techniques around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66df4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c6c7569",
   "metadata": {},
   "source": [
    "# Chapter 1:Text Sourcing & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60a92b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The main purpose of this section is to learn how to handle multiple Text sources.\n",
    "It is quite important how the text is sourced for Natural Language processing. In some examples it is easier to simply enter a string of text and process it. In real world examples however the Text needs to be sourced from multiple sources like \n",
    "website, HTML code, PDF, Word documents. This chapter will let you know how multiple texts are handled. Each coding cell is like a self contained Program with an executable code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc7010",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several functions that operate on strings. The full list can be found here.\n",
    "\n",
    "https://www.w3schools.com/python/python_ref_string.asp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2dce7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidential Material shall mean all material and information (such as material and information concerning Elisa’s business plans, finances, budgets, products and services, their technical implementations, systems, product development, inventions, research results, intellectual property rights, pricing, marketing, client, principal or partner relationships and prospects and other information concerning Elisa’s business), that Elisa furnishes or discloses to the Recipient regardless of whether the Confidential Material is disclosed in writing, orally or by any other means. Confidential Material need not be marked as confidential. Confidentiality obligations defined in this Undertaking shall extend to any Confidential Material which Elisa may have provided to the Recipient in connection with the Approved Purpose prior to the effective date of this Undertaking as of the date of its signature.\n",
      "\n",
      "\n",
      "The number of characters in this text are \n",
      "902\n",
      "C\n",
      ".\n",
      "al Ma\n",
      "13\n",
      "515\n",
      "592\n",
      "726\n",
      "28\n",
      "37\n",
      "55\n",
      "71\n",
      "89\n",
      "190\n",
      "194\n",
      "211\n",
      "230\n",
      "305\n",
      "388\n",
      "572\n",
      "613\n",
      "747\n",
      "['Confidential', 'Material', 'shall', 'mean', 'all', 'material', 'and', 'information', '(such', 'as', 'material', 'and', 'information', 'concerning', 'Elisa’s', 'business', 'plans,', 'finances,', 'budgets,', 'products', 'and', 'services,', 'their', 'technical', 'implementations,', 'systems,', 'product', 'development,', 'inventions,', 'research', 'results,', 'intellectual', 'property', 'rights,', 'pricing,', 'marketing,', 'client,', 'principal', 'or', 'partner', 'relationships', 'and', 'prospects', 'and', 'other', 'information', 'concerning', 'Elisa’s', 'business),', 'that', 'Elisa', 'furnishes', 'or', 'discloses', 'to', 'the', 'Recipient', 'regardless', 'of', 'whether', 'the', 'Confidential', 'Material', 'is', 'disclosed', 'in', 'writing,', 'orally', 'or', 'by', 'any', 'other', 'means.', 'Confidential', 'Material', 'need', 'not', 'be', 'marked', 'as', 'confidential.', 'Confidentiality', 'obligations', 'defined', 'in', 'this', 'Undertaking', 'shall', 'extend', 'to', 'any', 'Confidential', 'Material', 'which', 'Elisa', 'may', 'have', 'provided', 'to', 'the', 'Recipient', 'in', 'connection', 'with', 'the', 'Approved', 'Purpose', 'prior', 'to', 'the', 'effective', 'date', 'of', 'this', 'Undertaking', 'as', 'of', 'the', 'date', 'of', 'its', 'signature.']\n",
      "/n\n",
      "The number of words in this text are \n",
      "122\n",
      "The first word in this text is\n",
      "Confidential\n",
      "The first word in this text is\n",
      "signature.\n",
      "The word mean appears in position \n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#This is a simple line of code which defines a text and prints it.\n",
    "text = \"Confidential Material shall mean all material and information (such as material and information concerning Elisa’s business plans, finances, budgets, products and services, their technical implementations, systems, product development, inventions, research results, intellectual property rights, pricing, marketing, client, principal or partner relationships and prospects and other information concerning Elisa’s business), that Elisa furnishes or discloses to the Recipient regardless of whether the Confidential Material is disclosed in writing, orally or by any other means. Confidential Material need not be marked as confidential. Confidentiality obligations defined in this Undertaking shall extend to any Confidential Material which Elisa may have provided to the Recipient in connection with the Approved Purpose prior to the effective date of this Undertaking as of the date of its signature.\"\n",
    "print(text)\n",
    "print(\"\\n\")\n",
    "\n",
    "#Using the length function on a string\n",
    "print(\"The number of characters in this text are \")\n",
    "print(len(text))\n",
    "\n",
    "#Text is stored as an Array \n",
    "#Printing the first index=0\n",
    "print(text[0])\n",
    "#Printing the last index=0\n",
    "print(text[-1])\n",
    "#Printing the middle of the index\n",
    "print(text[10:15])\n",
    "\n",
    "#Print wherever M is appearing\n",
    "for i in range(0, len(text)):\n",
    "    if(text[i]==\"M\"):\n",
    "        print(i)\n",
    "\n",
    "#Print wherever m is appearing\n",
    "for i in range(0, len(text)):\n",
    "    if(text[i]==\"m\"):\n",
    "        print(i)\n",
    "\n",
    "#split the text into words\n",
    "text_words= text.split(' ')\n",
    "print(text_words)\n",
    "print(\"/n\")\n",
    "\n",
    "#print the number of words\n",
    "print(\"The number of words in this text are \")\n",
    "print(len(text_words))\n",
    "\n",
    "#print the first word\n",
    "print(\"The first word in this text is\")\n",
    "print(text_words[0])\n",
    "\n",
    "#print the last word\n",
    "print(\"The first word in this text is\")\n",
    "print(text_words[-1])\n",
    "\n",
    "#print a specific word\n",
    "\n",
    "\n",
    "print(\"The word mean appears in position \")\n",
    "for i in range(0, len(text_words)):\n",
    "    if(text_words[i]==\"mean\"):\n",
    "        print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea50ec0",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "    Text cleaning is a technique that developers use in a variety of domains. Depending on the goal of your project and where you get your data from, you may want to remove unwanted information, such as:\n",
    "\n",
    "Punctuation and accents\n",
    "Special characters\n",
    "Numeric digits\n",
    "Leading, ending, and vertical whitespace\n",
    "HTML formatting\n",
    "\n",
    "you can use the .sub() method in Python’s regular expression (re) library for most of your noise removal needs.\n",
    "The .sub() method has three required arguments:\n",
    "\n",
    "pattern – a regular expression that is searched for in the input string. There must be an r preceding the string to indicate it is a raw string, which treats backslashes as literal characters.\n",
    "replacement_text – text that replaces all matches in the input string\n",
    "input – the input string that will be edited by the .sub() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ccafc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    This is a paragraph\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    " \n",
    "text = \"<p>    This is a paragraph</p>\" \n",
    " \n",
    "result = re.sub(r'<.?p>', '', text)\n",
    " \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1c9511e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a paragraph\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    " \n",
    "text = \"    This is a paragraph\" \n",
    " \n",
    "result = re.sub(r'\\s{4}', '', text)\n",
    " \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd84f87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nation's Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini\n",
      "fat_meats, veggies are better than you think.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "headline_one = '<h1>Nation\\'s Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini</h1>'\n",
    "\n",
    "tweet = '@fat_meats, veggies are better than you think.'\n",
    "\n",
    "headline_no_tag= re.sub('<.?h1>',\"\",headline_one)\n",
    "\n",
    "\n",
    "tweet_no_at = re.sub(r'@',\"\",tweet)\n",
    "\n",
    "try:\n",
    "  print(headline_no_tag)\n",
    "except:\n",
    "  print('No variable called `headline_no_tag`')\n",
    "try:\n",
    "  print(tweet_no_at)\n",
    "except:\n",
    "  print('No variable called `tweet_no_at`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f67242",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "For many natural language processing tasks, we need access to each word in a string. To access each word, we first have to break the text into smaller components. The method for breaking text into smaller components is called tokenization and the individual components are called tokens.\n",
    "\n",
    "A few common operations that require tokenization include:\n",
    "\n",
    "Finding how many words or sentences appear in text\n",
    "Determining how many times a specific word or phrase exists\n",
    "Accounting for which terms are likely to co-occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07e146b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenize', 'this', 'text']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "text = \"Tokenize this text\"\n",
    "tokenized = word_tokenize(text)\n",
    " \n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc3bad41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenize this sentence.', 'Also, tokenize this sentence.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    " \n",
    "text = \"Tokenize this sentence. Also, tokenize this sentence.\"\n",
    "tokenized = sent_tokenize(text)\n",
    " \n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbeca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:\n",
      "['An', 'electrocardiogram', 'is', 'used', 'to', 'record', 'the', 'electrical', 'conduction', 'through', 'a', 'person', \"'s\", 'heart', '.', 'The', 'readings', 'can', 'be', 'used', 'to', 'diagnose', 'cardiac', 'arrhythmias', '.']\n",
      "Sentence Tokenization:\n",
      "[\"An electrocardiogram is used to record the electrical conduction through a person's heart.\", 'The readings can be used to diagnose cardiac arrhythmias.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "ecg_text = 'An electrocardiogram is used to record the electrical conduction through a person\\'s heart. The readings can be used to diagnose cardiac arrhythmias.'\n",
    "\n",
    "tokenized_by_word= word_tokenize(ecg_text)\n",
    "tokenized_by_sentence= sent_tokenize(ecg_text)\n",
    "\n",
    "try:\n",
    "  print('Word Tokenization:')\n",
    "  print(tokenized_by_word)\n",
    "except:\n",
    "  print('Expected a variable called `tokenized_by_word')\n",
    "try:\n",
    "  print('Sentence Tokenization:')\n",
    "  print(tokenized_by_sentence)\n",
    "except:\n",
    "  print('Expected a variable called `tokenized_by_sentence`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4e159",
   "metadata": {},
   "source": [
    "## Normalization \n",
    "Text normalization is a catch-all term for various text pre-processing tasks. In the next few exercises, we’ll cover a few of them:\n",
    "\n",
    "Upper or lowercasing\n",
    "Stopword removal\n",
    "Stemming – bluntly removing prefixes and suffixes from a word\n",
    "Lemmatization – replacing a single-word token with its root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7db04d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS HAS A MIX OF CASES\n",
      "this has a mix of cases\n"
     ]
    }
   ],
   "source": [
    "my_string = 'tHiS HaS a MiX oF cAsEs'\n",
    " \n",
    "print(my_string.upper())\n",
    " \n",
    "print(my_string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48665e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased brands: salvation army, ymca, boys & girls club of america\n",
      "Uppercased brands: SALVATION ARMY, YMCA, BOYS & GIRLS CLUB OF AMERICA\n"
     ]
    }
   ],
   "source": [
    "brands = 'Salvation Army, YMCA, Boys & Girls Club of America'\n",
    "\n",
    "brands_lower = brands.lower()\n",
    "brands_upper = brands.upper()\n",
    "\n",
    "try:\n",
    "  print(f'Lowercased brands: {brands_lower}')\n",
    "except:\n",
    "  print('Expected a variable called `brands_lower`')\n",
    "try:\n",
    "  print(f'Uppercased brands: {brands_upper}')\n",
    "except:\n",
    "  print('Expected a variable called `brands_upper`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b990aa",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "Stopwords are words that we remove during preprocessing when we don’t care about sentence structure. They are usually the most common words in a language and don’t provide any information about the tone of a statement. They include words such as “a”, “an”, and “the”.\n",
    "\n",
    "NLTK provides a built-in library with these words. You can import them using the following statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ee71089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9357d97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nithinku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c9f8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07a774d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NBC', 'founded', '1926', 'making', 'oldest', 'major', 'broadcast', 'network', 'USA']\n"
     ]
    }
   ],
   "source": [
    "nbc_statement = \"NBC was founded in 1926 making it the oldest major broadcast network in the USA\"\n",
    " \n",
    "word_tokens = word_tokenize(nbc_statement) \n",
    "# tokenize nbc_statement\n",
    " \n",
    "statement_no_stop = [word for word in word_tokens if word not in stop_words]\n",
    " \n",
    "print(statement_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2cbbc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'YouGov', 'study', 'found', 'American', \"'s\", 'like', 'Italian', 'food', 'country', \"'s\", 'cuisine', '.']\n",
      "Stopwords type: <class 'set'>\n",
      "Words Tokenized: ['A', 'YouGov', 'study', 'found', 'that', 'American', \"'s\", 'like', 'Italian', 'food', 'more', 'than', 'any', 'other', 'country', \"'s\", 'cuisine', '.']\n",
      "Text without Stops: ['A', 'YouGov', 'study', 'found', 'American', \"'s\", 'like', 'Italian', 'food', 'country', \"'s\", 'cuisine', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "survey_text = 'A YouGov study found that American\\'s like Italian food more than any other country\\'s cuisine.'\n",
    "\n",
    "tokenized_survey= word_tokenize(survey_text)\n",
    "\n",
    "stop_words= set(stopwords.words('english'))\n",
    "\n",
    "text_no_stops= [word for word in tokenized_survey if word not in stop_words]\n",
    "\n",
    "print(text_no_stops)\n",
    "\n",
    "try:\n",
    "  print(f'Stopwords type: {type(stop_words)}')\n",
    "except:\n",
    "  print('Expected a variable called `stop_words`')\n",
    "try:\n",
    "  print(f'Words Tokenized: {tokenized_survey}')\n",
    "except:\n",
    "  print('Expected a variable called `tokenized_survey`')\n",
    "try:\n",
    "  print(f'Text without Stops: {text_no_stops}')\n",
    "except:\n",
    "  print('Expected a variable called `text_no_stops`')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b258e8d6",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "In natural language processing, stemming is the text preprocessing normalization task concerned with bluntly removing word affixes (prefixes and suffixes). For example, stemming would cast the word “going” to “go”. This is a common method used by search engines to improve matching between user input and website hits.\n",
    "\n",
    "NLTK has a built-in stemmer called PorterStemmer. You can use it with a list comprehension to stem each word in a tokenized list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a6fd8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7fe4623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nbc', 'wa', 'found', 'in', '1926', '.', 'thi', 'make', 'nbc', 'the', 'oldest', 'major', 'broadcast', 'network', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized = ['NBC', 'was', 'founded', 'in', '1926', '.', 'This', 'makes', 'NBC', 'the', 'oldest', 'major', 'broadcast', 'network', '.']\n",
    " \n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    " \n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "acfa9922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A stemmer exists:\n",
      "<PorterStemmer>\n",
      "Words Tokenized:\n",
      "['Java', 'is', 'an', 'Indonesian', 'island', 'in', 'the', 'Pacific', 'Ocean', '.', 'It', 'is', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'people', '.']\n",
      "Stemmed Words:\n",
      "['java', 'is', 'an', 'indonesian', 'island', 'in', 'the', 'pacif', 'ocean', '.', 'It', 'is', 'the', 'most', 'popul', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'peopl', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "populated_island = 'Java is an Indonesian island in the Pacific Ocean. It is the most populated island in the world, with over 140 million people.'\n",
    "\n",
    "island_tokenized = word_tokenize(populated_island)\n",
    "\n",
    "stemmed = [stemmer.stem(token) for token in island_tokenized]\n",
    "\n",
    "try:\n",
    "  print('A stemmer exists:')\n",
    "  print(stemmer)\n",
    "except:\n",
    "  print('Expected a variable called `stemmer`')\n",
    "try:\n",
    "  print('Words Tokenized:')\n",
    "  print(island_tokenized)\n",
    "except:\n",
    "  print('Expected a variable called `island_tokenized`')\n",
    "try:\n",
    "  print('Stemmed Words:')\n",
    "  print(stemmed)\n",
    "except:\n",
    "  print('Expected a variable called `stemmed`')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9150bb9",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is a method for casting words to their root forms. This is a more involved process than stemming, because it requires the method to know the part-of-speech for each word. Since lemmatization requires the part of speech, it is a less efficient approach than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6418c795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10394f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NBC', 'wa', 'founded', 'in', '1926']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized = [\"NBC\", \"was\", \"founded\", \"in\", \"1926\"]\n",
    " \n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    " \n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8544e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lemmatizer exists: <WordNetLemmatizer>\n",
      "Words Tokenized: ['Indonesia', 'was', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n",
      "Lemmatized Words: ['Indonesia', 'wa', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(token) for token in tokenized_string]\n",
    "\n",
    "try:\n",
    "  print(f'A lemmatizer exists: {lemmatizer}')\n",
    "except:\n",
    "  print('Expected a variable called `lemmatizer`')\n",
    "try:\n",
    "  print(f'Words Tokenized: {tokenized_string}')\n",
    "except:\n",
    "  print('Expected a variable called `tokenized_string`')\n",
    "try:\n",
    "  print(f'Lemmatized Words: {lemmatized_words}')\n",
    "except:\n",
    "  print('Expected a variable called `lemmatized_words`')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db4c99a",
   "metadata": {},
   "source": [
    "### Part of speech tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9e56fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized words are: ['Indonesia', 'be', 'found', 'in', '1945', '.', 'It', 'contain', 'the', 'most', 'populate', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  \n",
    "  pos_counts = Counter()\n",
    "\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "\n",
    "lemmatized_pos=[lemmatizer.lemmatize(token,get_part_of_speech(token)) for token in tokenized_string]\n",
    "\n",
    "try:\n",
    "  print(f'The lemmatized words are: {lemmatized_pos}')\n",
    "except:\n",
    "  print('Expected a variable called `lemmatized_pos`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6da83fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-c8d06cfbec26>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-c8d06cfbec26>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    To the right, we added some HTML text that we pulled from Oprah Winfrey’s Wikipedia page. See if you can use only the skills you’ve learned in this lesson to:\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "To the right, we added some HTML text that we pulled from Oprah Winfrey’s Wikipedia page. See if you can use only the skills you’ve learned in this lesson to:\n",
    "\n",
    "Select only the string within the <p> tags.\n",
    "Remove all periods.\n",
    "Make all of the words lowercase.\n",
    "Tokenize the string.\n",
    "Lemmatize the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4315d8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working in local media, she was both the youngest news anchor and the first black female news anchor at Nashville's WLAC-TV. \n",
      "Working in local media, she was both the youngest news anchor and the first black female news anchor at Nashville's WLAC-TV. \n",
      "working in local media, she was both the youngest news anchor and the first black female news anchor at nashville's wlac-tv. \n",
      "['working', 'in', 'local', 'medium', ',', 'she', 'wa', 'both', 'the', 'youngest', 'news', 'anchor', 'and', 'the', 'first', 'black', 'female', 'news', 'anchor', 'at', 'nashville', \"'s\", 'wlac-tv', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "oprah_wiki = '<p>Working in local media, she was both the youngest news anchor and the first black female news anchor at Nashville\\'s WLAC-TV. </p>'\n",
    "\n",
    "oprah_no_tag= re.sub('<.?p>',\"\",oprah_wiki)\n",
    "\n",
    "print(oprah_no_tag)\n",
    "\n",
    "oprah_no_tag_no_period= re.sub('/.',\"\",oprah_no_tag)\n",
    "\n",
    "print(oprah_no_tag_no_period)\n",
    "\n",
    "oprah_lower= oprah_no_tag_no_period.lower()\n",
    "\n",
    "print(oprah_lower)\n",
    "\n",
    "oprah_tokenized = word_tokenize(oprah_lower)\n",
    "\n",
    "oprah_lemmatized = [lemmatizer.lemmatize(token) for token in oprah_tokenized]\n",
    "\n",
    "print(oprah_lemmatized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d47cf245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1176812"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "print(type(raw))\n",
    "print(raw[1])\n",
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f8e2d307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'eBook',\n",
       " 'of',\n",
       " 'Crime',\n",
       " 'and',\n",
       " 'Punishment',\n",
       " ',',\n",
       " 'by']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens= word_tokenize(raw)\n",
    "len(tokens)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c97f151b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.text.Text'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(type(text))\n",
    "text[1024:1062]\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f2dda",
   "metadata": {},
   "source": [
    "# https://www.nltk.org/book/ch03.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7234f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# characters are defined\n",
    "character_1 = \"Dorothy\"\n",
    "character_2 = \"Henry\"\n",
    "# compile your regular expression here\n",
    "regular_expression = re.compile(\"[A-Za-z]{7}\")\n",
    "# check for a match to character_1 here\n",
    "result_1 = regular_expression.match(character_1)\n",
    "match_1=(result_1.group(0))\n",
    "result_2 = re.match(\"[A-Za-z]{7}\",character_2)\n",
    "print(result_2)\n",
    "# store and print the matched text here\n",
    "\n",
    "\n",
    "\n",
    "# compile a regular expression to match a 7 character string of word characters and check for a match to character_2 here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2b881e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b9c6346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('do', 'VB'), ('you', 'PRP'), ('suppose', 'VB'), ('oz', 'NNS'), ('could', 'MD'), ('give', 'VB'), ('me', 'PRP'), ('a', 'DT'), ('heart', 'NN'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_sentence = ['do', 'you', 'suppose', 'oz', 'could', 'give', 'me', 'a', 'heart', '?']\n",
    "part_of_speech_tagged_sentence = pos_tag(word_sentence)\n",
    "print(part_of_speech_tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5e6041e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nithinku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nithinku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6199e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b71f87e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('do', 'VB'), ('you', 'PRP'), ('suppose', 'VB'), ('oz', 'NNS'), ('could', 'MD'), ('give', 'VB'), ('me', 'PRP'), ('a', 'DT'), ('heart', 'NN'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_sentence = ['do', 'you', 'suppose', 'oz', 'could', 'give', 'me', 'a', 'heart', '?']\n",
    "part_of_speech_tagged_sentence = pos_tag(word_sentence)\n",
    "print(part_of_speech_tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc04291a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please like and share our new page for our Indoor Trampoline Park Aftershock opening this fall\n",
      "['Please', 'like', 'and', 'share', 'our', 'new', 'page', 'for', 'our', 'Indoor', 'Trampoline', 'Park', 'Aftershock', 'opening', 'this', 'fall']\n",
      "[('Please', 'NNP'), ('like', 'IN'), ('and', 'CC'), ('share', 'NN'), ('our', 'PRP$'), ('new', 'JJ'), ('page', 'NN'), ('for', 'IN'), ('our', 'PRP$'), ('Indoor', 'NNP'), ('Trampoline', 'NNP'), ('Park', 'NNP'), ('Aftershock', 'NNP'), ('opening', 'VBG'), ('this', 'DT'), ('fall', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sample_text=\"\"\"\n",
    "Please like and share our new page for our Indoor Trampoline Park Aftershock opening this fall\"\"\"\n",
    "#The legend of the Ramayan is the most popular Indian epic.A lot of movies and serials have already\n",
    "#been shot in several languages here in India based on the Ramayana.\n",
    "\n",
    "print(sample_text)\n",
    "tokenized=nltk.sent_tokenize(sample_text)\n",
    "for i in tokenized:\n",
    "  words=nltk.word_tokenize(i)\n",
    "print(words)\n",
    "tagged_words=nltk.pos_tag(words)\n",
    "print(tagged_words)\n",
    "chunkGram=r\"\"\"VB: {}\"\"\"\n",
    "chunkParser=nltk.RegexpParser(chunkGram)\n",
    "chunked=chunkParser.parse(tagged_words)\n",
    "chunked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd71f28",
   "metadata": {},
   "source": [
    "### Spacy Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "602b3f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence\n",
      "Index:    [0, 1, 2, 3]\n",
      "Text:     ['This', 'is', 'a', 'sentence']\n",
      "is_alpha: [True, True, True, True]\n",
      "is_punct: [False, False, False, False]\n",
      "like_num: [False, False, False, False]\n",
      "This\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"This is a sentence\")\n",
    "print(doc.text)\n",
    "\n",
    "print(\"Index:   \", [token.i for token in doc])\n",
    "print(\"Text:    \", [token.text for token in doc])\n",
    "\n",
    "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc])\n",
    "print(\"like_num:\", [token.like_num for token in doc])\n",
    "\n",
    "first_token = doc[0]\n",
    "print(first_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca0db8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'other' has incorrect type (expected spacy.tokens.token.Token, got int)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a5839ca2157b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Get the next token in the document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mnext_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Check if the next token's text equals \"%\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m____\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"%\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Argument 'other' has incorrect type (expected spacy.tokens.token.Token, got int)"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.text:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.____ == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f346f6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114cf9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161a014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15991b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73b1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8d086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015ae39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c899b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3040aa8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140c2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6da413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d6230f5",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeaae5e",
   "metadata": {},
   "source": [
    "Regular expressions are special sequences of characters that describe a pattern of text that is to be matched\n",
    "We can use literals to match the exact characters that we desire\n",
    "Alternation, using the pipe symbol |, allows us to match the text preceding or following the |\n",
    "Character sets, denoted by a pair of brackets [], let us match one character from a series of characters\n",
    "Wildcards, represented by the period or dot ., will match any single character (letter, number, symbol or whitespace)\n",
    "Ranges allow us to specify a range of characters in which we can make a match\n",
    "Shorthand character classes like \\w, \\d and \\s represent the ranges representing word characters, digit characters, and whitespace characters, respectively\n",
    "Groupings, denoted with parentheses (), group parts of a regular expression together, and allows us to limit alternation to part of a regex\n",
    "Fixed quantifiers, represented with curly braces {}, let us indicate the exact quantity or a range of quantity of a character we wish to match\n",
    "Optional quantifiers, indicated by the question mark ?, allow us to indicate a character in a regex is optional, or can appear either 0 times or 1 time\n",
    "The Kleene star, denoted with the asterisk *, is a quantifier that matches the preceding character 0 or more times\n",
    "The Kleene plus, denoted by the plus +, matches the preceding character 1 or more times\n",
    "The anchor symbols hat ^ and dollar sign $ are used to match text at the start and end of a string, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33ba24e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "print(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb7d4109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(tags)\n",
    "grammar = 'exact: {<DT><JJ><NN><NN>}'\n",
    "parser = nltk.RegexpParser(grammar)\n",
    "result = parser.parse(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87348e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'help/tagsets/upenn_tagset.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\nithinku/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\nithinku\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\nithinku\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nithinku\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-35f162575701>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhelp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupenn_tagset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\help.py\u001b[0m in \u001b[0;36mupenn_tagset\u001b[1;34m(tagpattern)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mupenn_tagset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagpattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0m_format_tagset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"upenn_tagset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m#####################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\help.py\u001b[0m in \u001b[0;36m_format_tagset\u001b[1;34m(tagset, tagpattern)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_format_tagset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagpattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mtagdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"help/tagsets/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtagset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtagpattern\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0m_print_entries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'help/tagsets/upenn_tagset.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\nithinku/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\nithinku\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\nithinku\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nithinku\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "nltk.download()\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4358d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(tags)\n",
    "grammar = 'exact: {<DT><JJ><NN><NN>}'\n",
    "parser = nltk.RegexpParser(grammar)\n",
    "result = parser.parse(tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b940bfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
      "(S\n",
      "  (exact The/DT quick/JJ brown/NN fox/NN jumps/VBZ)\n",
      "  over/IN\n",
      "  the/DT\n",
      "  lazy/JJ\n",
      "  dog/NN)\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "print(tags)\n",
    "\n",
    "grammar = 'exact: {<DT><.*>*<VBZ>}'\n",
    "parser = nltk.RegexpParser(grammar)\n",
    "result = parser.parse(tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd1fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'waiting to hug you'\n",
    "tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(tags)\n",
    "grammar = 'exact: {<N.*>}'\n",
    "parser = nltk.RegexpParser(grammar)\n",
    "result = parser.parse(tags)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8288be2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'picture', 'of', 'dorian', 'gray', 'by', 'oscar', 'wilde', 'the', 'preface', 'the', 'artist', 'is', 'the', 'creator', 'of', 'beautiful', 'things', '.']\n",
      "[('to', 'TO'), ('reveal', 'VB'), ('art', 'NN'), ('and', 'CC'), ('conceal', 'VB'), ('the', 'DT'), ('artist', 'NN'), ('is', 'VBZ'), ('art', 'NN'), (\"'s\", 'POS'), ('aim', 'NN'), ('.', '.')]\n",
      "\n",
      "\n",
      "[((('i', 'NN'),), 962), ((('henry', 'NN'),), 200), ((('lord', 'NN'),), 197), ((('life', 'NN'),), 170), ((('harry', 'NN'),), 136), ((('dorian', 'JJ'), ('gray', 'NN')), 127), ((('something', 'NN'),), 126), ((('nothing', 'NN'),), 93), ((('basil', 'NN'),), 85), ((('the', 'DT'), ('world', 'NN')), 70), ((('everything', 'NN'),), 69), ((('anything', 'NN'),), 68), ((('hallward', 'NN'),), 68), ((('the', 'DT'), ('man', 'NN')), 61), ((('the', 'DT'), ('room', 'NN')), 60), ((('face', 'NN'),), 58), ((('the', 'DT'), ('door', 'NN')), 56), ((('love', 'NN'),), 55), ((('art', 'NN'),), 52), ((('course', 'NN'),), 51), ((('the', 'DT'), ('picture', 'NN')), 46), ((('the', 'DT'), ('lad', 'NN')), 45), ((('head', 'NN'),), 44), ((('round', 'NN'),), 44), ((('hand', 'NN'),), 44), ((('sibyl', 'NN'),), 41), ((('the', 'DT'), ('table', 'NN')), 40), ((('the', 'DT'), ('painter', 'NN')), 38), ((('sir', 'NN'),), 38), ((('a', 'DT'), ('moment', 'NN')), 38)]\n",
      "\n",
      "\n",
      "[((('i', 'NN'), ('am', 'VBP')), 101), ((('i', 'NN'), ('was', 'VBD')), 40), ((('i', 'NN'), ('want', 'VBP')), 37), ((('i', 'NN'), ('know', 'VBP')), 33), ((('i', 'NN'), ('have', 'VBP')), 32), ((('i', 'NN'), ('do', 'VBP'), (\"n't\", 'RB')), 31), ((('i', 'NN'), ('had', 'VBD')), 31), ((('i', 'NN'), ('suppose', 'VBP')), 17), ((('i', 'NN'), ('think', 'VBP')), 16), ((('i', 'NN'), ('am', 'VBP'), ('not', 'RB')), 14), ((('i', 'NN'), ('thought', 'VBD')), 13), ((('i', 'NN'), ('believe', 'VBP')), 12), ((('dorian', 'JJ'), ('gray', 'NN'), ('was', 'VBD')), 11), ((('i', 'NN'), ('am', 'VBP'), ('so', 'RB')), 11), ((('henry', 'NN'), ('had', 'VBD')), 11), ((('i', 'NN'), ('did', 'VBD'), (\"n't\", 'RB')), 9), ((('i', 'NN'), ('met', 'VBD')), 9), ((('i', 'NN'), ('said', 'VBD')), 9), ((('i', 'NN'), ('am', 'VBP'), ('quite', 'RB')), 8), ((('i', 'NN'), ('see', 'VBP')), 8), ((('i', 'NN'), ('did', 'VBD'), ('not', 'RB')), 7), ((('i', 'NN'), ('have', 'VBP'), ('ever', 'RB')), 7), ((('life', 'NN'), ('has', 'VBZ')), 7), ((('i', 'NN'), ('did', 'VBD')), 6), ((('i', 'NN'), ('feel', 'VBP')), 6), ((('life', 'NN'), ('is', 'VBZ')), 6), ((('the', 'DT'), ('lad', 'NN'), ('was', 'VBD')), 6), ((('i', 'NN'), ('asked', 'VBD')), 6), ((('i', 'NN'), ('came', 'VBD')), 6), ((('i', 'NN'), ('felt', 'VBD')), 6)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, RegexpParser\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "def word_sentence_tokenize(text):\n",
    "  \n",
    "  # create a PunktSentenceTokenizer\n",
    "  sentence_tokenizer = PunktSentenceTokenizer(text)\n",
    "  \n",
    "  # sentence tokenize text\n",
    "  sentence_tokenized = sentence_tokenizer.tokenize(text)\n",
    "  \n",
    "  # create a list to hold word tokenized sentences\n",
    "  word_tokenized = list()\n",
    "  \n",
    "  # for-loop through each tokenized sentence in sentence_tokenized\n",
    "  for tokenized_sentence in sentence_tokenized:\n",
    "    # word tokenize each sentence and append to word_tokenized\n",
    "    word_tokenized.append(word_tokenize(tokenized_sentence))\n",
    "    \n",
    "  return word_tokenized\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def np_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract noun phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def vp_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract verb phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import text of choice here\n",
    "text = open(\"dorian_gray.txt\",encoding='utf-8').read().lower()\n",
    "\n",
    "# sentence and word tokenize text here\n",
    "\n",
    "word_tokenized_text= word_sentence_tokenize(text)\n",
    "\n",
    "# store and print any word tokenized sentence here\n",
    "single_word_tokenized_sentence = word_tokenized_text [0]\n",
    "print(single_word_tokenized_sentence)\n",
    "\n",
    "# create a list to hold part-of-speech tagged sentences here\n",
    "\n",
    "pos_tagged_text = list()\n",
    "# create a for loop through each word tokenized sentence here\n",
    "\n",
    "for word in word_tokenized_text:\n",
    "  pos_tagged_text.append(pos_tag(word))\n",
    "\n",
    "  # part-of-speech tag each sentence and append to list of pos-tagged sentences here\n",
    "  \n",
    "\n",
    "# store and print any part-of-speech tagged sentence here\n",
    "\n",
    "single_pos_sentence = pos_tagged_text[1]\n",
    "print(single_pos_sentence)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# define noun phrase chunk grammar here\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# create noun phrase RegexpParser object here\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "\n",
    "# define verb phrase chunk grammar here\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "\n",
    "# create verb phrase RegexpParser object here\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)\n",
    "\n",
    "# create a list to hold noun phrase chunked sentences and a list to hold verb phrase chunked sentences here\n",
    "np_chunked_text = list()\n",
    "vp_chunked_text = list()\n",
    "\n",
    "\n",
    "\n",
    "# create a for loop through each pos-tagged sentence here\n",
    "for pos_tagged_sentence in pos_tagged_text:\n",
    "  # chunk each sentence and append to lists here\n",
    "  np_chunked_text.append(np_chunk_parser.parse(pos_tagged_sentence))\n",
    "  vp_chunked_text.append(vp_chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "# store and print the most common NP-chunks here\n",
    "\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "\n",
    "print(most_common_np_chunks)\n",
    "print(\"\\n\")\n",
    "\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "\n",
    "# store and print the most common VP-chunks here\n",
    "print(most_common_vp_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd8b23",
   "metadata": {},
   "source": [
    "## Different ways of processing text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bf7fc",
   "metadata": {},
   "source": [
    "### Importing from an URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7575db72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176812"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e3004831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1430d9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UK', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'Scientists', 'believe', 'the', 'last', 'blondes', 'will', 'be', 'in', 'Finland', 'The', 'last', 'natural', 'blondes', 'will', 'die', 'out', 'within', '200', 'years', ',', 'scientists', 'believe', '.', 'A', 'study', 'by', 'experts', 'in', 'Germany', 'suggests', 'people', 'with', 'blonde', 'hair', 'are', 'an', 'endangered', 'species', 'and', 'will', 'become', 'extinct', 'by', '2202', '.', 'Researchers', 'predict', 'the', 'last', 'truly', 'natural', 'blonde', 'will', 'be', 'born', 'in', 'Finland', '-', 'the', 'country', 'with', 'the', 'highest', 'proportion', 'of', 'blondes', '.', 'The', 'frequency', 'of', 'blondes', 'may', 'drop', 'but', 'they', 'wo', \"n't\", 'disappear', 'Prof', 'Jonathan', 'Rees', ',', 'University', 'of', 'Edinburgh', 'But', 'they', 'say', 'too', 'few', 'people', 'now', 'carry', 'the', 'gene', 'for', 'blondes', 'to', 'last', 'beyond', 'the', 'next', 'two', 'centuries', '.', 'The', 'problem', 'is', 'that', 'blonde', 'hair', 'is', 'caused', 'by', 'a', 'recessive', 'gene', '.', 'In', 'order', 'for', 'a', 'child', 'to', 'have', 'blonde', 'hair', ',', 'it', 'must', 'have', 'the', 'gene', 'on', 'both', 'sides', 'of', 'the', 'family', 'in', 'the', 'grandparents', \"'\", 'generation', '.', 'Dyed', 'rivals', 'The', 'researchers', 'also', 'believe', 'that', 'so-called', 'bottle', 'blondes', 'may', 'be', 'to', 'blame', 'for', 'the', 'demise', 'of', 'their', 'natural', 'rivals', '.', 'They', 'suggest', 'that', 'dyed-blondes', 'are', 'more', 'attractive', 'to', 'men', 'who', 'choose', 'them', 'as', 'partners', 'over', 'true', 'blondes', '.', 'Bottle-blondes', 'like', 'Ann', 'Widdecombe', 'may', 'be', 'to', 'blame', 'But', 'Jonathan', 'Rees', ',', 'professor', 'of', 'dermatology', 'at', 'the', 'University', 'of', 'Edinburgh', 'said', 'it', 'was', 'unlikely', 'blondes', 'would', 'die', 'out', 'completely', '.', '``', 'Genes', 'do', \"n't\", 'die', 'out', 'unless', 'there', 'is', 'a', 'disadvantage', 'of', 'having', 'that', 'gene', 'or', 'by', 'chance', '.', 'They', 'do', \"n't\", 'disappear', ',', \"''\", 'he', 'told', 'BBC', 'News', 'Online', '.', '``', 'The', 'only', 'reason', 'blondes', 'would', 'disappear', 'is', 'if', 'having', 'the', 'gene', 'was', 'a', 'disadvantage', 'and', 'I', 'do', 'not', 'think', 'that', 'is', 'the', 'case', '.', '``']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens[110:390])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "60382787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fellow-Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':', 'Among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.', 'On', 'the', 'one', 'hand', ',', 'I', 'was', 'summoned', 'by', 'my', 'Country', ',', 'whose', 'voice', 'I', 'can', 'never', 'hear', 'but', 'with', 'veneration', 'and', 'love', ',', 'from', 'a', 'retreat', 'which', 'I', 'had', 'chosen', 'with', 'the', 'fondest', 'predilection', ',', 'and', ',', 'in', 'my', 'flattering', 'hopes', ',', 'with', 'an', 'immutable', 'decision', ',', 'as', 'the', 'asylum']\n"
     ]
    }
   ],
   "source": [
    "f = open('1789-Washington.txt')\n",
    "raw = f.read()\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b031c61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fellow-Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':', 'Among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.', 'On', 'the', 'one', 'hand', ',', 'I', 'was', 'summoned', 'by', 'my', 'Country', ',', 'whose', 'voice', 'I', 'can', 'never', 'hear', 'but', 'with', 'veneration', 'and', 'love', ',', 'from', 'a', 'retreat', 'which', 'I', 'had', 'chosen', 'with', 'the', 'fondest', 'predilection', ',', 'and', ',', 'in', 'my', 'flattering', 'hopes', ',', 'with', 'an', 'immutable', 'decision', ',', 'as', 'the', 'asylum']\n"
     ]
    }
   ],
   "source": [
    "f = open('1789-Washington.txt')\n",
    "raw = f.read()\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ec051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac66a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf298c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb104bb1",
   "metadata": {},
   "source": [
    "### Bag of words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a70000e",
   "metadata": {},
   "source": [
    "Bag of words is a simple and powerful tool of NLP to determine topics, filtering Spam,sentiment analysis,wordclouds.\n",
    "It is based on wordcount and does not take into consideration the order or the context. It is also called as the Unigram model.\n",
    "One of the most common way of implementing Bag of words is to create a dictionary with each key set to a word and value set to the number of times that word appears. For statistical models, we call the text that we use to build the model our training data. Usually, we need to prepare our text data by breaking it up into documents (shorter strings of text, generally sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7fd64d",
   "metadata": {},
   "source": [
    "In this section the Prerocessing of the text occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aafe998",
   "metadata": {},
   "source": [
    "The text is taken in and then cleaned and each word appearing in the text is counted for unique values and a dictionary is created with a unique word and its count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "085e2eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 2, 'love': 1, 'fantastic': 2, 'fly': 2, 'fish': 3, 'these': 1, 'be': 1, 'just': 1, 'ok': 1, 'so': 1, 'maybe': 1, 'will': 1, 'find': 1, 'another': 1, 'few': 1}\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "def text_to_bow(text):\n",
    "    text_dict = {}\n",
    "    tokens= preprocess_text(text)\n",
    "    for token in tokens:\n",
    "        if token in text_dict:\n",
    "            text_dict[token]+=1 \n",
    "        else:      \n",
    "            text_dict[token]= 1\n",
    "    return text_dict\n",
    "\n",
    " \n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "  cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "  tokenized = word_tokenize(cleaned)\n",
    "  normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "  return normalized\n",
    "\n",
    "text=\"I love fantastic flying fish. These flying fish are just ok, so maybe I will find another few fantastic fish\"\n",
    "bow= text_to_bow(text)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72658bd1",
   "metadata": {},
   "source": [
    "The bag of words model then needs to be converted into a Vector format in order to be analyzed. These vectors are called as Feature Vectors.A feature vector is a numeric representation of an items important features. Each feature is a column represented by 1 for example.If a feature doesnt exist then it can be represented by 0. Turning Bag of words into a Vector is called as Feature extraction or Vectorization.\n",
    "When building BoW vectors we build Features dictionary of all Vocabulary in our training data(Text that is used to build data is called training data). \n",
    "\n",
    "For Example a Training data can be \n",
    "An eye for eye makes the whole world go blind\n",
    "The Features dictionary for this data can be\n",
    "an:1 eye:2 for:1 make:1 whole:1 world:1 go:1 blind:1\n",
    "an|0 eye|1 for|2 make|3 whole|4 world|5 go|6 blind|7\n",
    "\n",
    "\n",
    "If we pass another text(test data) for this vector\n",
    "Eye doctors treat the eye\n",
    "\n",
    "The Vector would look like\n",
    "0 1 2 3 4 5 6 7 \n",
    "0 2 0 0 0 0 0 0 \n",
    "\n",
    "It is important to remember \n",
    "\n",
    "For a bag of words \n",
    "Bag of Dictionary comes first \n",
    "Features Dictionary comes next \n",
    "Features Vector comes next\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "The below function creates a dictionary from a given list of documents.\n",
    "The documents are joined and preprocessing is applied.\n",
    "Then for each token in the document indices are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf727aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'five': 0, 'fantastic': 1, 'fish': 2, 'fly': 3, 'off': 4, 'to': 5, 'find': 6, 'faraway': 7, 'function': 8, 'maybe': 9, 'another': 10, 'my': 11, 'with': 12, 'a': 13, 'please': 14}, ['five', 'fantastic', 'fish', 'fly', 'off', 'to', 'find', 'faraway', 'function', 'maybe', 'find', 'another', 'five', 'fantastic', 'fish', 'find', 'my', 'fish', 'with', 'a', 'function', 'please'])\n"
     ]
    }
   ],
   "source": [
    "# Define create_features_dictionary() below:\n",
    "def create_features_dictionary(documents):\n",
    "  features_dictionary = {}\n",
    "  merged = \" \".join(documents)\n",
    "  tokens = preprocess_text(merged)\n",
    "  \n",
    "  index = 0\n",
    "  for token in tokens:\n",
    "    if token not in features_dictionary:\n",
    "      features_dictionary[token] = index\n",
    "      index += 1\n",
    "  return features_dictionary\n",
    "training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
    "\n",
    "print(create_features_dictionary(training_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8983450e",
   "metadata": {},
   "source": [
    "This function takes a text and a features dictionary as parameters and returns a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "327a84cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def text_to_bow_vector(some_text, features_dictionary):\n",
    "  bow_vector = [0] * len(features_dictionary)\n",
    "  tokens = preprocess_text(some_text)\n",
    "  for token in tokens:\n",
    "    feature_index = features_dictionary[token]\n",
    "    bow_vector[feature_index] += 1\n",
    "  return bow_vector, tokens\n",
    "\n",
    "features_dictionary = {'function': 8, 'please': 14, 'find': 6, 'five': 0, 'with': 12, 'fantastic': 1, 'my': 11, 'another': 10, 'a': 13, 'maybe': 9, 'to': 5, 'off': 4, 'faraway': 7, 'fish': 2, 'fly': 3}\n",
    "\n",
    "text = \"Another five fish find another faraway fish.\"\n",
    "print(text_to_bow_vector(text, features_dictionary)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c0b04",
   "metadata": {},
   "source": [
    "Once features dictionary function is ready the training documents are converted into vectors.\n",
    "The test documents are also converted into Vectors.\n",
    "Once the vectors are available they are passed passed into a proper Machine Learning algorithm for various classifications and results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c272d13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_features_dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f85d1596f2f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#Call the create features dictionary to get the dictionary created\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdictionary1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_features_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Call the vector functions to create the vectors from the training document and the test document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_features_dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "# In this simple function we will pass two text documents - One Training and one Test and run Naive Bayes classification on them.\n",
    "\n",
    "training_doc = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
    "test_doc =  \"Another five fish find another faraway fish.\"\n",
    "\n",
    "#Call the create features dictionary to get the dictionary created\n",
    "dictionary1 = create_features_dictionary(training_doc)\n",
    "\n",
    "# Call the vector functions to create the vectors from the training document and the test document\n",
    "training_vector = text_to_bow_vector(training_doc, dictionary1)\n",
    "test_vector = text_to_bow_vector(test_doc, dictionary1)\n",
    "\n",
    "print(training_vector)\n",
    "print(test_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ae947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f291762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ccb830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654cd71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4b56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60fb5927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8f14de4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-e70e92d32c6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4878048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\nithinku\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\nithinku\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\nithinku\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\nithinku\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\nithinku\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d76cd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1789-Washington.txt', '1793-Washington.txt', '1797-John-Adams.txt', '1801-Jefferson.txt', '1809-Madison.txt', '1813-Madison.txt', '1817-Monroe.txt', '1821-Monroe.txt', '1825-John-Q-Adams.txt', '1829-Jackson.txt', '1833-Jackson.txt', '1837-VanBuren.txt', '1841-William-Harrison.txt', '1845-Polk.txt', '1849-Taylor.txt', '1853-Pierce.txt', '1857-Buchanan.txt', '1861-Lincoln.txt', '1865-Lincoln.txt', '1869-Grant.txt', '1873-Grant.txt', '1877-Hayes.txt', '1881-Garfield.txt', '1885-Cleveland.txt', '1889-Benjamin-Harrison.txt', '1893-Cleveland.txt', '1897-McKinley.txt', '1901-McKinley.txt', '1905-Theodore-Roosevelt.txt', '1909-Taft.txt', '1913-Wilson.txt', '1917-Wilson.txt', '1921-Harding.txt', '1925-Coolidge.txt', '1929-Hoover.txt', '1933-Franklin-D-Roosevelt.txt', '1937-Franklin-D-Roosevelt.txt', '1941-Franklin-D-Roosevelt.txt', '1945-Franklin-D-Roosevelt.txt', '1949-Truman.txt', '1953-Eisenhower.txt', '1957-Eisenhower.txt', '1961-Kennedy.txt', '1965-Lyndon-Johnson.txt', '1969-Nixon.txt', '1973-Nixon.txt', '1977-Carter.txt', '1981-Reagan.txt', '1985-Reagan.txt', '1989-George-HW-Bush.txt', '1993-Clinton.txt', '1997-Clinton.txt', '2001-George-W-Bush.txt', '2005-George-W-Bush.txt', '2009-Obama.txt', '2013-Obama.txt', '2017-Trump.txt', 'dorian_gray.txt']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "import spacy\n",
    "from president_helper import read_file, process_speeches, merge_speeches, get_president_sentences, get_presidents_sentences, most_frequent_words\n",
    "\n",
    "# get list of all speech files\n",
    "files = sorted([file for file in os.listdir() if file[-4:] == '.txt'])\n",
    "print(files)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d97cb1",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Term frequency - Inverse document frequency\n",
    "Term frequency-inverse document frequency is a numerical statistic used to indicate how important a word is to each document in a collection of documents, or a corpus.\n",
    "\n",
    "When applying tf-idf to a corpus, each word is given a tf-idf score for each document, representing the relevance of that word to the particular document. A higher tf-idf score indicates a term is more important to the corresponding document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32cd6a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          this be a sample sentence  this be not my second sentence  \\\n",
      "be                         1.000000                        1.000000   \n",
      "my                         0.000000                        1.287682   \n",
      "not                        0.000000                        1.693147   \n",
      "sample                     1.693147                        0.000000   \n",
      "second                     0.000000                        1.693147   \n",
      "sentence                   1.000000                        1.000000   \n",
      "third                      0.000000                        0.000000   \n",
      "this                       1.000000                        1.000000   \n",
      "\n",
      "          be this my third sentence  \n",
      "be                         1.000000  \n",
      "my                         1.287682  \n",
      "not                        0.000000  \n",
      "sample                     0.000000  \n",
      "second                     0.000000  \n",
      "sentence                   1.000000  \n",
      "third                      1.693147  \n",
      "this                       1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "  cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "  tokenized = word_tokenize(cleaned)\n",
    "  normalized = \" \".join([normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized])\n",
    "  return normalized\n",
    "\n",
    "\n",
    "# sample documents\n",
    "document_1 = \"This is a sample sentence!\"\n",
    "document_2 = \"This is not my second sentence.\"\n",
    "document_3 = \"Is this my third sentence?\"\n",
    "\n",
    "\n",
    "# corpus of documents\n",
    "corpus = [document_1, document_2, document_3]\n",
    "\n",
    "# preprocess documents\n",
    "processed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tf_idf_scores = vectorizer.fit_transform(processed_corpus)\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "corpus_index = [n for n in processed_corpus]\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "df_tf_idf = pd.DataFrame(tf_idf_scores.T.todense(), index=feature_names, columns=corpus_index)\n",
    "print(df_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9de93907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1789-Washington.txt', '1793-Washington.txt', '1797-John-Adams.txt', '1801-Jefferson.txt', '1809-Madison.txt', '1813-Madison.txt', '1817-Monroe.txt', '1821-Monroe.txt', '1825-John-Q-Adams.txt', '1829-Jackson.txt', '1833-Jackson.txt', '1837-VanBuren.txt', '1841-William-Harrison.txt', '1845-Polk.txt', '1849-Taylor.txt', '1853-Pierce.txt', '1857-Buchanan.txt', '1861-Lincoln.txt', '1865-Lincoln.txt', '1869-Grant.txt', '1873-Grant.txt', '1877-Hayes.txt', '1881-Garfield.txt', '1885-Cleveland.txt', '1889-Benjamin-Harrison.txt', '1893-Cleveland.txt', '1897-McKinley.txt', '1901-McKinley.txt', '1905-Theodore-Roosevelt.txt', '1909-Taft.txt', '1913-Wilson.txt', '1917-Wilson.txt', '1921-Harding.txt', '1925-Coolidge.txt', '1929-Hoover.txt', '1933-Franklin-D-Roosevelt.txt', '1937-Franklin-D-Roosevelt.txt', '1941-Franklin-D-Roosevelt.txt', '1945-Franklin-D-Roosevelt.txt', '1949-Truman.txt', '1953-Eisenhower.txt', '1957-Eisenhower.txt', '1961-Kennedy.txt', '1965-Lyndon-Johnson.txt', '1969-Nixon.txt', '1973-Nixon.txt', '1977-Carter.txt', '1981-Reagan.txt', '1985-Reagan.txt', '1989-George-HW-Bush.txt', '1993-Clinton.txt', '1997-Clinton.txt', '2001-George-W-Bush.txt', '2005-George-W-Bush.txt', '2009-Obama.txt', '2013-Obama.txt', '2017-Trump.txt', 'dorian_gray.txt', 'preprocessing.py.txt']\n",
      "\n",
      "\n",
      "00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "\n",
      "1789-Washington.txt\n",
      "00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "\n",
      "1\n",
      "00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "\n",
      "8619\n",
      "First Item from the Corpus\n",
      "\n",
      "\n",
      "f\n",
      "\n",
      "\n",
      "Second Item from the Corpus\n",
      "\n",
      "\n",
      "e\n",
      "\n",
      "\n",
      "Third Item from the Corpus\n",
      "\n",
      "\n",
      "l\n",
      "\n",
      "\n",
      "Fourth Item from the Corpus\n",
      "\n",
      "\n",
      "l\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1789-Washington.txt\n",
      "59\n",
      "1789 washington txt\n",
      "  Are the tf-idf scores the same?\n",
      "0                             YES\n",
      "            Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "1789                1          0          0          0          0          0   \n",
      "1793                0          1          0          0          0          0   \n",
      "1797                0          0          1          0          0          0   \n",
      "1801                0          0          0          1          0          0   \n",
      "1809                0          0          0          0          1          0   \n",
      "...               ...        ...        ...        ...        ...        ...   \n",
      "txt                 1          1          1          1          1          1   \n",
      "vanburen            0          0          0          0          0          0   \n",
      "washington          1          1          0          0          0          0   \n",
      "william             0          0          0          0          0          0   \n",
      "wilson              0          0          0          0          0          0   \n",
      "\n",
      "            Article 7  Article 8  Article 9  Article 10  ...  Article 50  \\\n",
      "1789                0          0          0           0  ...           0   \n",
      "1793                0          0          0           0  ...           0   \n",
      "1797                0          0          0           0  ...           0   \n",
      "1801                0          0          0           0  ...           0   \n",
      "1809                0          0          0           0  ...           0   \n",
      "...               ...        ...        ...         ...  ...         ...   \n",
      "txt                 1          1          1           1  ...           1   \n",
      "vanburen            0          0          0           0  ...           0   \n",
      "washington          0          0          0           0  ...           0   \n",
      "william             0          0          0           0  ...           0   \n",
      "wilson              0          0          0           0  ...           0   \n",
      "\n",
      "            Article 51  Article 52  Article 53  Article 54  Article 55  \\\n",
      "1789                 0           0           0           0           0   \n",
      "1793                 0           0           0           0           0   \n",
      "1797                 0           0           0           0           0   \n",
      "1801                 0           0           0           0           0   \n",
      "1809                 0           0           0           0           0   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "txt                  1           1           1           1           1   \n",
      "vanburen             0           0           0           0           0   \n",
      "washington           0           0           0           0           0   \n",
      "william              0           0           0           0           0   \n",
      "wilson               0           0           0           0           0   \n",
      "\n",
      "            Article 56  Article 57  Article 58  Article 59  \n",
      "1789                 0           0           0           0  \n",
      "1793                 0           0           0           0  \n",
      "1797                 0           0           0           0  \n",
      "1801                 0           0           0           0  \n",
      "1809                 0           0           0           0  \n",
      "...                ...         ...         ...         ...  \n",
      "txt                  1           1           1           1  \n",
      "vanburen             0           0           0           0  \n",
      "washington           0           0           0           0  \n",
      "william              0           0           0           0  \n",
      "wilson               0           0           0           0  \n",
      "\n",
      "[104 rows x 59 columns]\n",
      "            Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "1789         4.401197   0.000000   0.000000   0.000000   0.000000        0.0   \n",
      "1793         0.000000   4.401197   0.000000   0.000000   0.000000        0.0   \n",
      "1797         0.000000   0.000000   4.401197   0.000000   0.000000        0.0   \n",
      "1801         0.000000   0.000000   0.000000   4.401197   0.000000        0.0   \n",
      "1809         0.000000   0.000000   0.000000   0.000000   4.401197        0.0   \n",
      "...               ...        ...        ...        ...        ...        ...   \n",
      "txt          1.000000   1.000000   1.000000   1.000000   1.000000        1.0   \n",
      "vanburen     0.000000   0.000000   0.000000   0.000000   0.000000        0.0   \n",
      "washington   3.995732   3.995732   0.000000   0.000000   0.000000        0.0   \n",
      "william      0.000000   0.000000   0.000000   0.000000   0.000000        0.0   \n",
      "wilson       0.000000   0.000000   0.000000   0.000000   0.000000        0.0   \n",
      "\n",
      "            Article 7  Article 8  Article 9  Article 10  ...  Article 50  \\\n",
      "1789              0.0        0.0        0.0         0.0  ...         0.0   \n",
      "1793              0.0        0.0        0.0         0.0  ...         0.0   \n",
      "1797              0.0        0.0        0.0         0.0  ...         0.0   \n",
      "1801              0.0        0.0        0.0         0.0  ...         0.0   \n",
      "1809              0.0        0.0        0.0         0.0  ...         0.0   \n",
      "...               ...        ...        ...         ...  ...         ...   \n",
      "txt               1.0        1.0        1.0         1.0  ...         1.0   \n",
      "vanburen          0.0        0.0        0.0         0.0  ...         0.0   \n",
      "washington        0.0        0.0        0.0         0.0  ...         0.0   \n",
      "william           0.0        0.0        0.0         0.0  ...         0.0   \n",
      "wilson            0.0        0.0        0.0         0.0  ...         0.0   \n",
      "\n",
      "            Article 51  Article 52  Article 53  Article 54  Article 55  \\\n",
      "1789               0.0         0.0         0.0         0.0         0.0   \n",
      "1793               0.0         0.0         0.0         0.0         0.0   \n",
      "1797               0.0         0.0         0.0         0.0         0.0   \n",
      "1801               0.0         0.0         0.0         0.0         0.0   \n",
      "1809               0.0         0.0         0.0         0.0         0.0   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "txt                1.0         1.0         1.0         1.0         1.0   \n",
      "vanburen           0.0         0.0         0.0         0.0         0.0   \n",
      "washington         0.0         0.0         0.0         0.0         0.0   \n",
      "william            0.0         0.0         0.0         0.0         0.0   \n",
      "wilson             0.0         0.0         0.0         0.0         0.0   \n",
      "\n",
      "            Article 56  Article 57  Article 58  Article 59  \n",
      "1789               0.0         0.0         0.0         0.0  \n",
      "1793               0.0         0.0         0.0         0.0  \n",
      "1797               0.0         0.0         0.0         0.0  \n",
      "1801               0.0         0.0         0.0         0.0  \n",
      "1809               0.0         0.0         0.0         0.0  \n",
      "...                ...         ...         ...         ...  \n",
      "txt                1.0         1.0         1.0         1.0  \n",
      "vanburen           0.0         0.0         0.0         0.0  \n",
      "washington         0.0         0.0         0.0         0.0  \n",
      "william            0.0         0.0         0.0         0.0  \n",
      "wilson             0.0         0.0         0.0         0.0  \n",
      "\n",
      "[104 rows x 59 columns]\n",
      "            Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "1789         4.401197   0.000000   0.000000   0.000000   0.000000        0.0   \n",
      "1793         0.000000   4.401197   0.000000   0.000000   0.000000        0.0   \n",
      "1797         0.000000   0.000000   4.401197   0.000000   0.000000        0.0   \n",
      "1801         0.000000   0.000000   0.000000   4.401197   0.000000        0.0   \n",
      "1809         0.000000   0.000000   0.000000   0.000000   4.401197        0.0   \n",
      "...               ...        ...        ...        ...        ...        ...   \n",
      "txt          1.000000   1.000000   1.000000   1.000000   1.000000        1.0   \n",
      "vanburen     0.000000   0.000000   0.000000   0.000000   0.000000        0.0   \n",
      "washington   3.995732   3.995732   0.000000   0.000000   0.000000        0.0   \n",
      "william      0.000000   0.000000   0.000000   0.000000   0.000000        0.0   \n",
      "wilson       0.000000   0.000000   0.000000   0.000000   0.000000        0.0   \n",
      "\n",
      "            Article 7  Article 8  Article 9  Article 10  ...  Article 50  \\\n",
      "1789              0.0        0.0        0.0         0.0  ...         0.0   \n",
      "1793              0.0        0.0        0.0         0.0  ...         0.0   \n",
      "1797              0.0        0.0        0.0         0.0  ...         0.0   \n",
      "1801              0.0        0.0        0.0         0.0  ...         0.0   \n",
      "1809              0.0        0.0        0.0         0.0  ...         0.0   \n",
      "...               ...        ...        ...         ...  ...         ...   \n",
      "txt               1.0        1.0        1.0         1.0  ...         1.0   \n",
      "vanburen          0.0        0.0        0.0         0.0  ...         0.0   \n",
      "washington        0.0        0.0        0.0         0.0  ...         0.0   \n",
      "william           0.0        0.0        0.0         0.0  ...         0.0   \n",
      "wilson            0.0        0.0        0.0         0.0  ...         0.0   \n",
      "\n",
      "            Article 51  Article 52  Article 53  Article 54  Article 55  \\\n",
      "1789               0.0         0.0         0.0         0.0         0.0   \n",
      "1793               0.0         0.0         0.0         0.0         0.0   \n",
      "1797               0.0         0.0         0.0         0.0         0.0   \n",
      "1801               0.0         0.0         0.0         0.0         0.0   \n",
      "1809               0.0         0.0         0.0         0.0         0.0   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "txt                1.0         1.0         1.0         1.0         1.0   \n",
      "vanburen           0.0         0.0         0.0         0.0         0.0   \n",
      "washington         0.0         0.0         0.0         0.0         0.0   \n",
      "william            0.0         0.0         0.0         0.0         0.0   \n",
      "wilson             0.0         0.0         0.0         0.0         0.0   \n",
      "\n",
      "            Article 56  Article 57  Article 58  Article 59  \n",
      "1789               0.0         0.0         0.0         0.0  \n",
      "1793               0.0         0.0         0.0         0.0  \n",
      "1797               0.0         0.0         0.0         0.0  \n",
      "1801               0.0         0.0         0.0         0.0  \n",
      "1809               0.0         0.0         0.0         0.0  \n",
      "...                ...         ...         ...         ...  \n",
      "txt                1.0         1.0         1.0         1.0  \n",
      "vanburen           0.0         0.0         0.0         0.0  \n",
      "washington         0.0         0.0         0.0         0.0  \n",
      "william            0.0         0.0         0.0         0.0  \n",
      "wilson             0.0         0.0         0.0         0.0  \n",
      "\n",
      "[104 rows x 59 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gensim\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "  cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "  tokenized = word_tokenize(cleaned)\n",
    "  normalized = \" \".join([normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized])\n",
    "  return normalized\n",
    "\n",
    "\n",
    "articles = sorted([file for file in os.listdir() if file[-4:] == '.txt'])\n",
    "\n",
    "print(articles)\n",
    "print(\"\\n\")\n",
    "print(\"00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(articles[0])\n",
    "print(\"00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(articles[0][0])\n",
    "print(\"00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\")\n",
    "print(\"\\n\")\n",
    "\n",
    "corpus = open(str(articles[0]),encoding='utf-8').read().lower()\n",
    "#for i in range(0,len(articles)): \n",
    " #corpus[i]=open(articles[i],encoding='utf-8').read().lower()\n",
    "\n",
    "\n",
    "print(len(corpus))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"First Item from the Corpus\")\n",
    "print(\"\\n\")\n",
    "print(corpus[0][0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Second Item from the Corpus\")\n",
    "print(\"\\n\")\n",
    "print(corpus[1])\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Third Item from the Corpus\")\n",
    "print(\"\\n\")\n",
    "print(corpus[2])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Fourth Item from the Corpus\")\n",
    "print(\"\\n\")\n",
    "print(corpus[3])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\")\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# view article\n",
    "print(articles[0])\n",
    "print(len(articles))\n",
    "\n",
    "# preprocess articles\n",
    "processed_articles = [preprocess_text(article) for article in articles]\n",
    "print(processed_articles[0])\n",
    "\n",
    "\n",
    "# initialize and fit CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "counts = vectorizer.fit_transform(processed_articles)\n",
    "\n",
    "\n",
    "# convert counts to tf-idf\n",
    "transformer = TfidfTransformer(norm=None)\n",
    "tfidf_scores_transformed = transformer.fit_transform(counts)\n",
    "\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tfidf_scores = vectorizer.fit_transform(processed_articles)\n",
    "\n",
    "# check if tf-idf scores are equal\n",
    "\n",
    "if np.allclose(tfidf_scores_transformed.todense(), tfidf_scores.todense()):\n",
    "  print(pd.DataFrame({'Are the tf-idf scores the same?':['YES']}))\n",
    "else:\n",
    "  print(pd.DataFrame({'Are the tf-idf scores the same?':['No, something is wrong :(']}))\n",
    "\n",
    "# get vocabulary of terms\n",
    "try:\n",
    "  feature_names = vectorizer.get_feature_names_out()\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# get article index\n",
    "try:\n",
    "  article_index = [f\"Article {i+1}\" for i in range(len(articles))]\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# create pandas DataFrame with word counts\n",
    "try:\n",
    "  df_word_counts = pd.DataFrame(counts.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_word_counts)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# create pandas DataFrame(s) with tf-idf scores\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores_transformed.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b4163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c173c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
